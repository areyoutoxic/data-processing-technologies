{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в обработку текста на естественном языке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Материалы:\n",
    "* Макрушин С.В. Лекция 9: Введение в обработку текста на естественном языке\\\n",
    "* https://realpython.com/nltk-nlp-python/\n",
    "* https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи для совместного разбора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''с велечайшим усилием выбравшись из потока убегающих людей Кутузов со свитой уменьшевшейся вдвое поехал на звуки выстрелов русских орудий'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Считайте слова из файла `litw-win.txt` и запишите их в список `words`. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка `words`. Считайте, что в слове есть опечатка, если данное слово не содержится в списке `words`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "с велечайшим усилием выбравшись из потока убегающих людей Кутузов со свитой уменьшевшейся вдвое поехал на звуки выстрелов русских орудий\n",
      "с величайшим усилием выбравшись из потока убегающих людей кутузов со свитой уменьшившейся вдвое поехал на звуки выстрелов русских орудий\n"
     ]
    }
   ],
   "source": [
    "with open(\"litw-win.txt\", \"r\") as f:\n",
    "    words = f.read().split()\n",
    "\n",
    "def levenshtein_distance(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]\n",
    "\n",
    "\n",
    "def correct_typo(word, words_list):\n",
    "    min_distance = float(\"inf\")\n",
    "    closest_word = word\n",
    "\n",
    "    for w in words_list:\n",
    "        distance = levenshtein_distance(word, w)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_word = w\n",
    "\n",
    "    return closest_word\n",
    "\n",
    "corrected_text = []\n",
    "for word in text.split():\n",
    "    if word in words:\n",
    "        corrected_text.append(word)\n",
    "    else:\n",
    "        corrected_text.append(correct_typo(word, words))\n",
    "\n",
    "corrected_text = \" \".join(corrected_text)\n",
    "print(text)\n",
    "print(corrected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Разбейте текст из формулировки задания 1 на слова; проведите стемминг и лемматизацию слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2\n",
      "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
      "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
      "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
      "Collecting docopt>=0.6\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Collecting dawg-python>=0.7.1\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=dc0b8a695284fe062af5df40c045100a58b11cf25b716afcdfb0f9b1ff86b978\n",
      "  Stored in directory: c:\\users\\marya\\appdata\\local\\pip\\cache\\wheels\\70\\4a\\46\\1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "Successfully built docopt\n",
      "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pymorphy2 import MorphAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['с', 'величайш', 'усил', 'выбра', 'из', 'поток', 'убега', 'люд', 'кутуз', 'со', 'свит', 'уменьш', 'вдво', 'поеха', 'на', 'звук', 'выстрел', 'русск', 'оруд']\n",
      "['с', 'великий', 'усилие', 'выбраться', 'из', 'поток', 'убегать', 'человек', 'кутузов', 'с', 'свита', 'уменьшиться', 'вдвое', 'поехать', 'на', 'звук', 'выстрел', 'русский', 'орудие']\n"
     ]
    }
   ],
   "source": [
    "words = corrected_text.split()\n",
    "\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "lemmatized_words = [morph.parse(word)[0].normal_form for word in words]\n",
    "\n",
    "print(stemmed_words)\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Преобразуйте предложения из формулировки задания 1 в векторы при помощи `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['вдвое' 'величайшим' 'выбравшись' 'выстрелов' 'звуки' 'из' 'кутузов'\n",
      " 'людей' 'на' 'орудий' 'поехал' 'потока' 'русских' 'свитой' 'со'\n",
      " 'убегающих' 'уменьшившейся' 'усилием']\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([corrected_text])\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расстояние редактирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Загрузите предобработанные описания рецептов из файла `preprocessed_descriptions.csv`. Получите набор уникальных слов `words`, содержащихся в текстах описаний рецептов (воспользуйтесь `word_tokenize` из `nltk`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.metrics.distance import edit_distance\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('recipes_sample.csv')\n",
    "descriptions = df['description'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set()\n",
    "for description in descriptions:\n",
    "    if pd.isna(description):\n",
    "        continue\n",
    "    tokens = word_tokenize(description)\n",
    "    words.update(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Сгенерируйте 5 пар случайно выбранных слов и посчитайте между ними расстояние редактирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расстояние редактирования между словами \"honorable\" и \"pot-lucks\" равно 8\n",
      "Расстояние редактирования между словами \"1950s\" и \"hates\" равно 4\n",
      "Расстояние редактирования между словами \"collapse\" и \"finished\" равно 8\n",
      "Расстояние редактирования между словами \"turban\" и \"fyi\" равно 6\n",
      "Расстояние редактирования между словами \"butternut\" и \"parcels\" равно 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marya\\AppData\\Local\\Temp\\ipykernel_3952\\1417854365.py:3: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  word1, word2 = random.sample(words, 2)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    word1, word2 = random.sample(words, 2)\n",
    "    distance = edit_distance(word1, word2)\n",
    "    print(f'Расстояние редактирования между словами \"{word1}\" и \"{word2}\" равно {distance}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Напишите функцию, которая для заданного слова `word` возвращает `k` ближайших к нему слов из списка `words` (близость слов измеряется с помощью расстояния Левенштейна)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_words(word, words, k):\n",
    "    distances = [(edit_distance(word, w), w) for w in words]\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    closest_words = [w[1] for w in distances[:k]]\n",
    "    return closest_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 ближайших слов к 'processing': ['processing', 'proceeding', 'pressing', 'processor', 'preceding']\n"
     ]
    }
   ],
   "source": [
    "word = \"processing\"\n",
    "k = 5\n",
    "closest_words = find_closest_words(word, words, 5)\n",
    "print(f\"{k} ближайших слов к '{word}': {closest_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг, лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 На основе результатов 1.1 создайте `pd.DataFrame` со столбцами: \n",
    "    * word\n",
    "    * stemmed_word \n",
    "    * normalized_word \n",
    "\n",
    "Столбец `word` укажите в качестве индекса. \n",
    "\n",
    "Для стемминга воспользуйтесь `SnowballStemmer`, для нормализации слов - `WordNetLemmatizer`. Сравните результаты стемминга и лемматизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\marya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\marya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "data = []\n",
    "for word in words:\n",
    "    stemmed_word = stemmer.stem(word)\n",
    "    normalized_word = lemmatizer.lemmatize(word)\n",
    "    data.append([word, stemmed_word, normalized_word])\n",
    "\n",
    "df_words = pd.DataFrame(data, columns=['word', 'stemmed_word', 'normalized_word'])\n",
    "df_words.set_index('word', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        stemmed_word    normalized_word\n",
      "word                                                   \n",
      "sewing                           sew             sewing\n",
      "berry-and-yoghurt  berry-and-yoghurt  berry-and-yoghurt\n",
      "aus/nz                        aus/nz             aus/nz\n",
      "customising                 customis        customising\n",
      "dillman                      dillman            dillman\n",
      "away                            away               away\n",
      "las                              las                 la\n",
      "babyfit                      babyfit            babyfit\n",
      "set-up                        set-up             set-up\n",
      "mini-rolls                  mini-rol         mini-rolls\n"
     ]
    }
   ],
   "source": [
    "print(df_words.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Удалите стоп-слова из описаний рецептов. Какую долю об общего количества слов составляли стоп-слова? Сравните топ-10 самых часто употребляемых слов до и после удаления стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля стоп-слов: 40.26%\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_descriptions = []\n",
    "for description in descriptions:\n",
    "    if pd.isna(description):\n",
    "        continue\n",
    "    tokens = word_tokenize(description)\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    filtered_description = ' '.join(filtered_tokens)\n",
    "    filtered_descriptions.append(filtered_description)\n",
    "\n",
    "total_words = sum(len(word_tokenize(description)) for description in descriptions if not pd.isna(description))\n",
    "total_filtered_words = sum(len(word_tokenize(description)) for description in filtered_descriptions)\n",
    "stop_words_ratio = (total_words - total_filtered_words) / total_words\n",
    "print(f'Доля стоп-слов: {stop_words_ratio:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-10 самых часто употребляемых слов до удаления стоп-слов:\n",
      ".: 66140\n",
      "the: 40257\n",
      ",: 38544\n",
      "a: 35030\n",
      "and: 30425\n",
      "i: 27799\n",
      "this: 27132\n",
      "to: 23508\n",
      "it: 23212\n",
      "is: 20501\n",
      "Топ-10 самых часто употребляемых слов после удаления стоп-слов:\n",
      ".: 66233\n",
      ",: 38544\n",
      "!: 16054\n",
      "recipe: 15121\n",
      "'s: 7689\n",
      "make: 6367\n",
      "``: 5470\n",
      "time: 5198\n",
      "n't: 4798\n",
      "use: 4645\n"
     ]
    }
   ],
   "source": [
    "all_words = [word for description in descriptions if not pd.isna(description) for word in word_tokenize(description)]\n",
    "all_filtered_words = [word for description in filtered_descriptions for word in word_tokenize(description)]\n",
    "\n",
    "top_words = Counter(all_words).most_common(10)\n",
    "top_filtered_words = Counter(all_filtered_words).most_common(10)\n",
    "\n",
    "print('Топ-10 самых часто употребляемых слов до удаления стоп-слов:')\n",
    "for word, count in top_words:\n",
    "    print(f'{word}: {count}')\n",
    "\n",
    "print('Топ-10 самых часто употребляемых слов после удаления стоп-слов:')\n",
    "for word, count in top_filtered_words:\n",
    "    print(f'{word}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторное представление текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Выберите случайным образом 5 рецептов из набора данных. Представьте описание каждого рецепта в виде числового вектора при помощи `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Описание рецепта: this is a great hot appetizer recipe. i made this for a dinner party and everyone loved it. i have even made it using reduced fat ingredients and it is still very tasty. it is from taste of home magazine and is credited to linda wheeler.\n",
      "Вектор: [0.         0.         0.34698816 0.11566272 0.         0.\n",
      " 0.         0.         0.11566272 0.11566272 0.         0.11566272\n",
      " 0.11566272 0.11566272 0.11566272 0.11566272 0.         0.09331595\n",
      " 0.         0.11566272 0.11566272 0.11566272 0.         0.11566272\n",
      " 0.46265088 0.46265088 0.         0.         0.         0.\n",
      " 0.         0.11566272 0.         0.11566272 0.         0.23132544\n",
      " 0.11566272 0.         0.         0.         0.         0.\n",
      " 0.11566272 0.         0.         0.09331595 0.         0.11566272\n",
      " 0.11566272 0.         0.         0.         0.         0.\n",
      " 0.         0.11566272 0.         0.         0.         0.\n",
      " 0.11566272 0.11566272 0.         0.         0.23132544 0.\n",
      " 0.07746067 0.         0.         0.         0.11566272 0.11566272\n",
      " 0.11566272 0.        ]\n",
      "Описание рецепта: nummmy !!!! coffee lovers take note, can be addictive.\n",
      "Вектор: [0.35355339 0.         0.         0.         0.35355339 0.\n",
      " 0.35355339 0.35355339 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.35355339 0.\n",
      " 0.         0.         0.         0.         0.35355339 0.35355339\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.35355339 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Описание рецепта: kids love these! great to serve the morning after a slumber party or on a lazy sunday morning. takes no time to put together!\n",
      "Вектор: [0.         0.20808112 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.16787854\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.20808112 0.20808112 0.\n",
      " 0.         0.         0.20808112 0.         0.         0.\n",
      " 0.         0.         0.41616224 0.20808112 0.         0.\n",
      " 0.         0.20808112 0.20808112 0.16787854 0.20808112 0.\n",
      " 0.         0.         0.20808112 0.         0.20808112 0.\n",
      " 0.         0.         0.20808112 0.         0.         0.20808112\n",
      " 0.         0.         0.20808112 0.20808112 0.         0.20808112\n",
      " 0.2787087  0.20808112 0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Описание рецепта: just like grandma used to make\n",
      "Вектор: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.42841136 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.42841136 0.         0.         0.42841136\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.42841136 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.28691208 0.         0.         0.42841136 0.         0.\n",
      " 0.         0.        ]\n",
      "Описание рецепта: sirloin steak tacos in soft tortilla with guacamole, beans, lime,salsas, etc...\n",
      "Вектор: [0.         0.         0.         0.         0.         0.28867513\n",
      " 0.         0.         0.         0.         0.28867513 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.28867513 0.         0.         0.         0.28867513 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.28867513 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.28867513 0.         0.28867513 0.         0.28867513\n",
      " 0.28867513 0.         0.         0.28867513 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.28867513 0.         0.         0.\n",
      " 0.         0.28867513]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "random_rows = df.sample(5)\n",
    "random_descriptions = random_rows['description'].tolist()\n",
    "random_names = random_rows['name'].tolist()\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(random_descriptions)\n",
    "tfidf_vectors = tfidf_matrix.toarray()\n",
    "\n",
    "for i, description in enumerate(random_descriptions):\n",
    "    vector = tfidf_vectors[i]\n",
    "    print(f'Описание рецепта: {description}')\n",
    "    print(f'Вектор: {vector}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Вычислите близость между каждой парой рецептов, выбранных в задании 3.1, используя косинусное расстояние (`scipy.spatial.distance.cosine`) Результаты оформите в виде таблицы `pd.DataFrame`. В качестве названий строк и столбцов используйте названия рецептов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Близость между каждой парой рецептов:\n",
      "                           sausage quiche squares  cappuccino ice  wake up waffle sandwiches  chicken paprikash  mexican carne asada tacos\n",
      "sausage quiche squares                   1.000000             0.0                   0.052920           0.022224                        0.0\n",
      "cappuccino ice                           0.000000             1.0                   0.000000           0.000000                        0.0\n",
      "wake up waffle sandwiches                0.052920             0.0                   1.000000           0.079965                        0.0\n",
      "chicken paprikash                        0.022224             0.0                   0.079965           1.000000                        0.0\n",
      "mexican carne asada tacos                0.000000             0.0                   0.000000           0.000000                        1.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "similarity_matrix = []\n",
    "for i in range(len(tfidf_vectors)):\n",
    "    row = []\n",
    "    for j in range(len(tfidf_vectors)):\n",
    "        similarity = 1 - distance.cosine(tfidf_vectors[i], tfidf_vectors[j])\n",
    "        row.append(similarity)\n",
    "    similarity_matrix.append(row)\n",
    "\n",
    "df_similarity = pd.DataFrame(similarity_matrix, index=random_names, columns=random_names)\n",
    "\n",
    "print('Близость между каждой парой рецептов:')\n",
    "print(df_similarity.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Какие рецепты являются наиболее похожими? Прокомментируйте результат (словами)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наиболее похожими являются рецепты “texas chili” и “baked spanish risotto”, т.к. значение близости между ними является наибольшим значением в таблице (0.222029). Описания этих рецептов имеют наибольшее сходство среди всех выбранных рецептов."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bbc3c3d932324566a9bf4b4a52ddf64063695fc3adbf25b3fda92572428493bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
